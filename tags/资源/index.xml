<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>资源 on Askyx's Blog</title><link>https://askyx.github.io/tags/%E8%B5%84%E6%BA%90/</link><description>Recent content in 资源 on Askyx's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-Hans</language><lastBuildDate>Wed, 20 Jul 2022 17:44:29 +0800</lastBuildDate><atom:link href="https://askyx.github.io/tags/%E8%B5%84%E6%BA%90/index.xml" rel="self" type="application/rss+xml"/><item><title>数据库文章资源汇总</title><link>https://askyx.github.io/posts/static/note/</link><pubDate>Wed, 20 Jul 2022 17:44:29 +0800</pubDate><guid>https://askyx.github.io/posts/static/note/</guid><description>爬虫 import requests from bs4 import BeautifulSoup prefix = &amp;#39;http://mysql.taobao.org&amp;#39; # 获取文章名和url（文章名,url） def query_name_url(url: str): resp = requests.get(url) soup = BeautifulSoup(resp.content.decode(&amp;#39;utf-8&amp;#39;), &amp;#34;html.parser&amp;#34;) tags = soup.findAll(&amp;#39;a&amp;#39;, {&amp;#39;target&amp;#39;: {&amp;#39;_top&amp;#39;}}) urls = [v for v in tags if v[&amp;#39;href&amp;#39;].find(&amp;#39;/monthly/&amp;#39;) != -1] return [(str(v.string).strip(), prefix + v[&amp;#39;href&amp;#39;]) for v in urls] # 获取所有</description></item></channel></rss>